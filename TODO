----- STREAM OF CONSCIOUSNESS TODO FILE --------------------------------
The contents of this file may not make sense to anyone (including me).
Proceed at your own peril.
------------------------------------------------------------------------


O. add multiprocessing.RLock to blacklist database calls
    O. remove database/* locks
O. instead of communicating with messages through a queue,
    just pass self.blacklist & have messages make blacklist calls
O. refactor Reddit instantiation
    O. move to reddit.py (maybe rename redditprefix -> reddit)
    O. refactor bot.py:_handle_rate_limit -> reddit.py
        O. create a generic error handler which calls the pertinent
          error-handler (map APIException handlers by error_type)
        O. need to notify all processes of rate-limit & put all to sleep
    O. instantiate in bot.__init__
    O. instantiate another instance in messages.run_forever
O. add config options:
    O. SEND_DEBUG_PM on/off
        O. refactor into separate class? (in replies.py maybe?)
            -- needs to instantiate its own Reddit instance (to get the
            redditor object)
        ? add more debug pms
X. move messages.py reply methods to replies.py .. maybe
    I don't think anything else should need those specific methods
    (but the same can probably said of the methods in replies.py)
O. mentions parsing
    O. queue.put( thing.submission )
    O. gate by time -- don't parse through the same submission within
        T time
          but there could be new comments with usable links
          someone could spam mentions in a single submission
            eg. hitting enter a bunch of times because of lag
        O. maybe keep a database of (username, submission) & only
            queue.put if the specific combination has not been seen
            (prevents the same user summoning multiple times)
O. refactor messages, mentions stream parsing to mixin/base class
    O. change ProcessBase -> ProcessMixin
    O. handle prawcore.exceptions.RequestException (internet hiccup, etc)
O. implement bot.subs
    O. read subreddits from database / json
    O. join subreddits with '+'
    O. update from database whenever the file mtime changes
        (this is required for mentions-added subreddits to work without
         bot restart)
O. refactor database to use context-management (with statements) & commit/rollback
O. instagram stuff
    O. fetching & caching (+config option to expire cache)
        O. parse data into usable format
        O. caching = save data to file .. maybe a database? json?
            O. sqlite3 caching = check against file mtime
    O. bad links db for temp bans (404s only?)
        => if #bad_links > THRESHOLD: temp ban user!
    O. if ig fetch was triggered by mentions (submission_queue)
        and ig fetch was successful (fetch ok or cached)
        then add subreddit to to-add db:
            if subreddit reaches a threshold of successful mention summons
            then add subreddit to subreddit.comment stream parsing (bot.py)
O. instagram rate-limit queue
    O. needs database (ig_user, comment.fullname, last_id*, timestamp**)
      *last_id may be null if rate-limited before any fetches were made
        otherwise this should be the last fetched id (data['items'][-1]['id'])
      **timestamp could also just be an INTEGER AUTOINCREMENT
        O. how to treat table as queue?
            > SELECT ... FROM ... ORDER BY timestamp ASC;
    O. instagram inserts on rate-limit + other?
    O. bot processes every so often? alternatively, instagram processes queue
      whenever under rate-limit (needs a way to call appropriate bot reply
      func tho... or could instantiate a Reddit instance i guess..)
        O. refactor instagram.__*rate_limit* methods to public staticmethods
          so that bot.py can see them & use them to process queue
        O. whatever handles consuming queue needs to handle that ig_user hitting
          rate-limit again:
            > UPDATE ... (last_id)
        O. on successful fetch:
            > DELETE ...
X. catch-up (parse through old (all?) messages maybe comments/mentions)
    in case the bot is offline for a while, this would catch messages to be
    processed
    - refactor messages to fetch until first not-seen message
        (I think stream_generator only fetches the first 100 then continually
         tries to fetch the newest)
    ? refactor mentions/comments similarly
        this behavior may not be desirable; could end up responding to weeks-old
        comments to be seen by no one
O. commandline arguments
    . --config-path
    . --add-subreddit
    . --rm-subreddit
    . --add-blacklist
    . --rm-blacklist
    . --lookup-*
        database lookups; not sure which atm .. maybe all?
10? delay comment replies with persistent queue
    it seems like some people dislike bots because they reply with information
    that is useless to them. delaying the reply could potentially mitigate some
    hate.
X. don't dynamically add subreddits if they are blacklisted
    >>> this cannot happen (reply must occur in order for a subreddit to be
            added)
O. instagram requests user-agent
O. versioning
O. refactor logging
    O. add pid
    ? special config handling required?
    - bare-bones formatter
        need to refactor handle_special_keyword so that Formatter child classes
        can extend/override
    O. actually refactor logger calls in app
O. refactor reddit rate-limit handling to queue callbacks rather than sleeping
    this way, the bot will miss less comments to reply to
    assumption: POST/GET are separate rate-limits; ie, replying may be rate-
        limited but can still fetch new comments.
    O. persistent queue (adds a lot of complexity for a rare situation)
        requires a larger refactor (need to queue comment ?)
        X. plug into ig-queue
        O. table(thing_id, body_text, rate_limit_expire_time_utc)
        O. insert on rate-limit (or if rate-limit is active)
            (update if already queued -- ie, hit rate-limit again)
        O. delete on successful processing
        X. need in-memory queue?
    O. need to handle hitting rate-limit again from queue
    O. does this make error_handling obsolete? yes
O. logger: prepend msg with exc_info type name, ie:
            '.'.join([ exc_info.__module__, exc_info.__class__.__name__ ])
    > ideally, after any id information
O. __metaclass__ py2/3 compatibility
    (added six dependency which I think requests already depends on)
O. split config into sections
    O. write default config in repo
    O. read default config & merge with existing (if exists) & write config
        basically, don't squash existing options but do merge in newly defined
        options (and create a default config if it doesn't exist)
- remove (all?) database paths from config
    why even allow the user to change these?
- dynamically update config settings
    - config.reload (check mtime vs cached mtime)
- write pids to RUNTIME_ROOT_DIR
    - os.makedirs(RUNTIME_ROOT_DIR)
- initialize logger from config / args
    - initialize immediately to streamhandler
    - once config loaded, initialize with config settings
- --dump: handle instagram databases
- change submission_queue from Queue -> database
    in case the program terminates before the queue can be processed
- catch UniqueConstraintFailed
    - grep '[.]insert('
    - grep '[.]update('
- use logger {yesno} keyword
- reclassify logger.exceptions
- graceful exit
    - logging.flush (or whatever the method is)
    - database.close? .flush?

N. testing!

