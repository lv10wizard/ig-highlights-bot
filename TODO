----- STREAM OF CONSCIOUSNESS TODO FILE --------------------------------
The contents of this file may not make sense to anyone (including me).
Proceed at your own peril.
------------------------------------------------------------------------


O. add multiprocessing.RLock to blacklist database calls
    O. remove database/* locks
O. instead of communicating with messages through a queue,
    just pass self.blacklist & have messages make blacklist calls
O. refactor Reddit instantiation
    O. move to reddit.py (maybe rename redditprefix -> reddit)
    O. refactor bot.py:_handle_rate_limit -> reddit.py
        O. create a generic error handler which calls the pertinent
          error-handler (map APIException handlers by error_type)
        O. need to notify all processes of rate-limit & put all to sleep
    O. instantiate in bot.__init__
    O. instantiate another instance in messages.run_forever
O. add config options:
    O. SEND_DEBUG_PM on/off
        O. refactor into separate class? (in replies.py maybe?)
            -- needs to instantiate its own Reddit instance (to get the
            redditor object)
        ? add more debug pms
X. move messages.py reply methods to replies.py .. maybe
    I don't think anything else should need those specific methods
    (but the same can probably said of the methods in replies.py)
O. mentions parsing
    O. queue.put( thing.submission )
    O. gate by time -- don't parse through the same submission within
        T time
          but there could be new comments with usable links
          someone could spam mentions in a single submission
            eg. hitting enter a bunch of times because of lag
        O. maybe keep a database of (username, submission) & only
            queue.put if the specific combination has not been seen
            (prevents the same user summoning multiple times)
O. refactor messages, mentions stream parsing to mixin/base class
    O. change ProcessBase -> ProcessMixin
    O. handle prawcore.exceptions.RequestException (internet hiccup, etc)
O. implement bot.subs
    O. read subreddits from database / json
    O. join subreddits with '+'
    O. update from database whenever the file mtime changes
        (this is required for mentions-added subreddits to work without
         bot restart)
O. refactor database to use context-management (with statements) & commit/rollback
O. instagram stuff
    O. fetching & caching (+config option to expire cache)
        O. parse data into usable format
        O. caching = save data to file .. maybe a database? json?
            O. sqlite3 caching = check against file mtime
    O. bad links db for temp bans (404s only?)
        => if #bad_links > THRESHOLD: temp ban user!
    O. if ig fetch was triggered by mentions (submission_queue)
        and ig fetch was successful (fetch ok or cached)
        then add subreddit to to-add db:
            if subreddit reaches a threshold of successful mention summons
            then add subreddit to subreddit.comment stream parsing (bot.py)
O. instagram rate-limit queue
    O. needs database (ig_user, comment.fullname, last_id*, timestamp**)
      *last_id may be null if rate-limited before any fetches were made
        otherwise this should be the last fetched id (data['items'][-1]['id'])
      **timestamp could also just be an INTEGER AUTOINCREMENT
        O. how to treat table as queue?
            > SELECT ... FROM ... ORDER BY timestamp ASC;
    O. instagram inserts on rate-limit + other?
    O. bot processes every so often? alternatively, instagram processes queue
      whenever under rate-limit (needs a way to call appropriate bot reply
      func tho... or could instantiate a Reddit instance i guess..)
        O. refactor instagram.__*rate_limit* methods to public staticmethods
          so that bot.py can see them & use them to process queue
        O. whatever handles consuming queue needs to handle that ig_user hitting
          rate-limit again:
            > UPDATE ... (last_id)
        O. on successful fetch:
            > DELETE ...
X. catch-up (parse through old (all?) messages maybe comments/mentions)
    in case the bot is offline for a while, this would catch messages to be
    processed
    - refactor messages to fetch until first not-seen message
        (I think stream_generator only fetches the first 100 then continually
         tries to fetch the newest)
    ? refactor mentions/comments similarly
        this behavior may not be desirable; could end up responding to weeks-old
        comments to be seen by no one
O. commandline arguments
    . --config-path
    . --add-subreddit
    . --rm-subreddit
    . --add-blacklist
    . --rm-blacklist
    . --lookup-*
        database lookups; not sure which atm .. maybe all?
10? delay comment replies with persistent queue
    it seems like some people dislike bots because they reply with information
    that is useless to them. delaying the reply could potentially mitigate some
    hate.
X. don't dynamically add subreddits if they are blacklisted
    >>> this cannot happen (reply must occur in order for a subreddit to be
            added)
O. instagram requests user-agent
O. versioning
O. refactor logging
    O. add pid
    ? special config handling required?
    - bare-bones formatter
        need to refactor handle_special_keyword so that Formatter child classes
        can extend/override
    O. actually refactor logger calls in app
O. refactor reddit rate-limit handling to queue callbacks rather than sleeping
    this way, the bot will miss less comments to reply to
    assumption: POST/GET are separate rate-limits; ie, replying may be rate-
        limited but can still fetch new comments.
    O. persistent queue (adds a lot of complexity for a rare situation)
        requires a larger refactor (need to queue comment ?)
        X. plug into ig-queue
        O. table(thing_id, body_text, rate_limit_expire_time_utc)
        O. insert on rate-limit (or if rate-limit is active)
            (update if already queued -- ie, hit rate-limit again)
        O. delete on successful processing
        X. need in-memory queue?
    O. need to handle hitting rate-limit again from queue
    O. does this make error_handling obsolete? yes
O. logger: prepend msg with exc_info type name, ie:
            '.'.join([ exc_info.__module__, exc_info.__class__.__name__ ])
    > ideally, after any id information
O. __metaclass__ py2/3 compatibility
    (added six dependency which I think requests already depends on)
O. split config into sections
    O. write default config in repo
    O. read default config & merge with existing (if exists) & write config
        basically, don't squash existing options but do merge in newly defined
        options (and create a default config if it doesn't exist)
O. remove (all?) database paths from config
    why even allow the user to change these?
O. dynamically update config settings
    O. config.reload (check mtime vs cached mtime)
O. write pids to RUNTIME_ROOT_DIR
    O. os.makedirs(RUNTIME_ROOT_DIR)
    O. check if pid file(s?) exist before starting processes
    O. modify readme: 'mac' -> 'osx' & 'Application Support' -> 'Preferences'
    X. defer reddit login
O. initialize logger from config / args
    O. initialize immediately to streamhandler
    O. once config loaded, initialize with config settings
    O. no-color logging setting
    O. check config logging level is valid
        X. set up level lookup dictionary in logger/methods
O. fix python2 database (+more?) import
    dies on blacklist trying to import reddit
O. --dump: handle instagram databases
O. README:
    O. include section explaining config time parsing
    O. explain what EMAIL is used for (instagram user-agent string)
    O. change 'ig user\'s' -> 'instagram user\'s'
    O. change praw '> 4.0' -> '>= 4.0'
    O. document all locations that the program stores files
O. --remove-all-files (or similar)
    delete all stored files (ie, uninstall)
O. change submission_queue from Queue -> database
    in case the program terminates before the queue can be processed
O. catch UniqueConstraintFailed
    grep -iE '[.]insert\(|[.]update\(' -R *
O. use logger {yesno} keyword
O. reclassify logger.exceptions
O. graceful exit
    O. logging.shutdown
    X. database.close? .flush?
O. refactor queue processing to separate process so that comment streams are
  never interrupted (also in case comment streams always have new data --
  ie, no pause in comment streams)
    O. turn bot, mentions processes into queue producers which filter comments
      that we want to reply to (ie, that have instagram user links & can_reply)
        O. refactor can_reply logic to separate module/class
            O. pass/instantiate reply_history db in mentions proc
                ... or maybe in whatever houses can_reply
            O. make enqueue() method (called by bot and mentions)
            O. make get() method (called by consumer)
        O. refactor submission_queue (probably no longer needed)
    O. create new process which consumes queue:
      (this process should be the only one handling outbound replies so that
       the inbound streams are never interrupted)
        O. do_reply
        X. process ig_queue if reply_queue empty
        O. dynamically add subreddits
        O. temporarily ban bad actors
            O. mentions process needs to increment bad actors if summoned to
              post with no instagram links
                (possible false positive: comment contains non-hyperlinked
                 links -- eg. 'instagram: @foo.bar')
        ? send debug pms?
O. catch "sqlite3.OperationalError: database is locked"
    this occurs when 2+ procs attempt to execute on the same database at the
    same time (thrown on timeout I think)
    X. reraise as DatabaseIsLocked or something
        >> just auto-retry until it gets the lock
    O. double check any database that auto-updates is catching:
        O. IntegrityError
        O. OperationalError (with _db: ...)
            O. catch on _database level (loop until successful I guess)

O. why do reddit's rate_limit_time.value and ratelimit's rate_limit_time.value
    differ?
    > reddit kept trying to queue the reply => (value - now) <= 0
    >> ratelimit time remaining calculation was backwards

O. instantiate reddit instances in separate process (for debugging)
O. persistent reddit ratelimit time
    O. write end time to file and read in on init
    O. if time.time() > value => remove file
      else set rate_limit_time.value
? maximum depth of reply-able comment
    ie, only reply to comments where c.depth <= max_depth. typically
    very deep comments are only seen by the author & the person they were
    replying to (and maybe a handful of others); bot replies are probably not
    welcome unless specifically summoned.
? check comment's immediate replies in Filter to guarantee that the bot does not
    make duplicate replies regardless of database state
O. gracefully exit instagram fetch
    O. pass _killed flag to Instagram class? __init__?
    O. in fetch, if _killed.is_set() => set do_enqueue
O? figure out why messages stream sleeps upon fetching items
    is it only if #items < 100?
    >> pretty sure this is praw ratelimit sleeping
O. reclassify some debug logging -> info (info is far too quiet)
X. add the bot-posted comment id to the [Contact] msg subject skeleton
    X. get comment from thing.reply()
    O. change CONTACT_SUBJECT_SKELETON to include a comment id placeholder
        (ie, '{0}')
    X. comment.edit( ... )
        probably: comment.edit( comment.body.format(comment.id) )
    * what if ratelimited on .edit ?
    >> formatting in parent permalink so no need to .edit
O. quiet spammy logging
    X. reddit-queue.db
        X. SELECT * FROM queue ORDER BY ratelimit_reset ASC
        X. Sleeping timeout=...
    X. reply-queue.db SELECT count(*)
    >> handled at base Database level by limiting the elapsed time between
        the same logged query.
- Fetch all(?) messages/mentions on first run and stop on first duplicate
    (to prevent missing older items)
O. delete negative threshold comments (ie, downvote to remove bot comments)
    O. fetch bot's comments:
        _reddit.user.me().comments.controversial(time_filter='month')
        .. maybe time_filter='week'
    O. call comment.delete() if comment.score < threshold
    O. define delete threshold in config
    O. dynamically change delay based on if any comments had negative score
        - saw non-deleted negative: delay = lower =>  O(minutes) ?
        - all comment.scores > 0:   delay = higher => O(hours) ?
    ! DO NOT modify reply database (don't want to reply again)
O. Write missing config options on startup
O. write help post & link appropriately in code (formatter.py)
O. change replyable_usernames to return list to preverve order
O. include choices in --help (add to help='...' msg)
- handle database table restructuring without dropping all data
O. parse soft-linked users
    O. in comments: @foo.bar
    X. in submission title: @foo.bar
        > requires another database?
            need to prevent reparsing same submission constantly
        >> too much extra overhead for no real gain
            99% of submissions don't soft-link anyway
    O. disable 404 badactor flagging since the bot is now guessing at usernames
        .. basically badactor flagging is turned off completely after this
O. test Parser.ig_{links,usernames}
    O. create script that takes comment_id & filename to create pickles
        O. c = Comment(comment_id)
        O. c.body
        O. with open('filename.py{ver}.pickle', 'wb') as fd:
            pickle.dump(c, fd)
    O. with usernames
    O. without usernames
    O. with links
    O. without links
    O. without either
O. add num_comments column to InstagramDatabase
    want a way to include #comments in sort but not all media has comments

    X. highlights = ORDER BY num_likes DESC, num_comments DESC
        should get highest liked & most commented media first (ie, most popular)
        >> this would just sort by num_likes anyway ...
    ? include time so that new posts that are very popular are sorted correctly
        - cap the impact of time at ~3d?
            (so that very old posts that were popular aren't sorted to the end)
    O. change args.py --ig-db ORDER BY to reflect new order
    O. change FAQ, bot side-bar "top-liked" -> "most popular"

O. ignore AutoModerator '@username' soft-links
    ! hard-links ok
    O. fix IG_USER_REGEX to match '(@username)'
    O. in ig_usernames: skip if comment.author.name.lower() == 'automoderator'
        or better: ignore if .name.lower() in IGNORE_LIST
    O. test: parenthesis_username.pickle

O? determine why there is a 3 ~ 5s delay between the Replier removing the comment
    from the repy-queue and calling ._reply() when instagram data is fetched
    (just looking up from database does not incur this delay)
    > is this delay system dependant?
    >> pretty sure this is related to praw sleeping
X. reply-queue per-comment logging
    > how to prevent spam the log though?
    >> handled in 821f2ea (kind of)
? refactor ratelimit flag process -> thread
    > do multiprocessing.Events work in threads?
        need to test; I assume yes
? logger keyword: {pluralize}
    - take tuple? list? needs the actual word value (eg. 'entry') and the
        corresponding num kwarg to test if the word should be plural
    - if ends in 'y':
        - check word[0] == upper => 's'
        - check word[-2] != vowel => 'ies'
    (in case a database is changed significantly)
- turn off Parser._get_potential_user_strings() if bot is scraping a popular
  subreddit
    - how to classify a subreddit as popular?
        > maybe average(?) time between comments?
? move instagram urls to separate file?
- refactor instagram to package
    the file is getting too large
    - ratelimit handler
    - fetch handler
    ? queue handler
    ? database handler
? refactor parser to package ?
? store last controversial check iteration time/delay in file
    so that program restarts don't re-trigger an early controversial check
O. --shutdown or --kill or something so that the program can be terminated from
    any directory/terminal
O. why is 'Daring' considered jargon?
O. match 's?he\'s {username} on insta'
    X. make prefix/suffix a single list of fmt
    X. make helper func that string.formats each fmt in list for
        HAS_IG_KEYWORD, IG_USER_STRING
    >> broadened how matching works

- fix instagram 500-level code delay logging spam
    > can't handle at replier level because users may be cached
        but sort of have to handle there since spam starts at that level

O. try finding usernames per line
    O. [IG_USER_STRING_REGEX.search(text.strip()) for text in comment.body.split('\n')]
        X. filter(None, ...)
        >> no need
O. parse submission titles for possible instagram user strings
    O. parse submission .url?
    O. parse submission .selftext?
    O. parse submission .title
    O. submissions process?
        > can't rely on every submission having a comment
    X. need database to store submissions
        >> just re-parse; shouldn't be a huge issue
    O. store replies properly
        ? refactor replies.db ?
            > I think the column name, comment_fullname, is wrong but should
                work besides that
    O. update faq
    O. update sidebar
    O. update readme
    O. update app description
O. parse submission from mentions
O. parse comments' submissions from comment stream if not parsed yet?
? separate submissions/comments min length for potential username matches
- dynamically update logging level/color (maybe path?) when config is reloaded
- add most liked, most commented links to reply header
    > how to handle duplicates?
        1. most liked == most commented
            - fetch most liked from db
            - fetch most commented with exclusions from db
                (exclude=most_liked_code)
        2. either/both already in highlights set
            - fetch top_media exclude=(most_liked, most_commented)
O. learn "bad" words so that they are not tried again
    O. prevent previously deleted comment users from being matched as
        potential usernames in the future (links still ok)
        > what about '@'s?
            1. non-instagram user soft-link
            2. are there other cases?
            >> previous bad usernames will prune any non-link username
        O. ''.join(map(lambda x: x+'+', bad_username))
            eg. 'foobar' -> 'f+o+o+b+a+r+'
            >> may as well match against these
X. --dry-run Controversial
    X. don't delete thing
    >> the bot should probably delete regardless of mode
- fix test_parser creating actual databases
O. blacklist subredits the bot has been banned from
    X. read SubredditMessages saying "banned from /r/..."
        >> just checking .user_is_banned property
    O. remove subreddit
        O. from database
        O. from SUBREDDITS file
O. cancel fetch if insta user has < 1k followers
    O. make 1k a config option?
O. post link to private profiles if:
    O. no link was given
        O. '@username', 'ig: ...', '... on ig', and/or 'taken-by=username' links
            >> not handling 'taken-by=username' links
        O. DO NOT link string usernames
        X. handle 'taken-by=username' links
            >> I'm not convinced there is any point in linking this case
    O. user has >= N followers (N=1k)
    O. differentiate private profiles in instagram database
        O. add 'is_private' property to Instagram class
? reply to summoner instead of trigger comment(s)
- use .executemany in InstagramDatabase._delete
? differentiate between non-existant IG user & too few followers
    both are just flagged as BAD right now
O. post highlights of users to profile
    O. choose from list (randomly(?))
        O. text file list? database?
            > maybe text file for user to write lines to
                database updates from file based on mtime
        O. need to ensure the same user is not posted too often
            (ie don't post user back-to-back basically)
        O. submit link to random(?) non-highlighted media
    O. do not post if profile becomes private/non-existant
    O. do not post too often
        O. config-based post interval
    O. handle reddit ratelimiting

    > to post: (should probably go in src/reddit.py)
        # maybe read subreddit from config (default to 'u_{botusername}')
        subreddit = self._reddit.subreddit('u_{0}'.format(self._reddit.username_raw))
        try:
            # http://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html#praw.models.Subreddit.submit
            # !!! this needs to be wrapped in _network_wrapper
            subreddit.submit( ... )

        except APIException as e:
            # handle 'SUBREDDIT_NOEXIST' in case the subreddit doesn't exist
            #   > kill subreddit.submit process if this happens
            # handle RATELIMIT
            ...

- how to handle users with 10k+ posts?
    - 5k+
    - 3k+
    - 1k+
- determine why reply-queue was wiped 2017/11/09.121928.log
    (2017/11/07.162045.log -> 09.121928.log wiped)
    > was it corrupted somehow?

- determine why Fetcher._handle_rate_limit does not always properly reset
  the ratelimit timing
    > has something to do with how the state change
        (was_ratelimited -> not is_ratelimited) is detected

? change SUBREDDITS, USER_POOL files to config files?
    - copy repo version to config directory
    - read config directory version > repo version
    > how to handle extra from repo version?
        - update with extra -- but what if user deleted those lines?
O. fix secondary process issuing instagram request when already ratelimited
O. add mention author to footer when replying from a mention
- refactor reddit.do_* methods so that there isn't so much duplication
    ? refactor _enqueue: the _queue_* methods are somewhat duplicated
O. --backup, --load-backup
    O. _db.iterdump(), read in from file when --load-backup
    O. refactor confirmation -> util
? fix broken links by HEAD-ing them?
    - if 404 -> continue down table
? refactor RedditRateLimitQueueDatabase so that it contains 1 table per
    thing (replies, post submissions, and PMs)

- take post date into account when considering like/comment count
    - interpolate follower count with cubic spline
      ** may need a better interpolation function
        https://en.wikipedia.org/wiki/Cubic_Hermite_spline
        https://stackoverflow.com/a/31544486

        from scipy import interpolate
        timestamps = [ ... ]
        num_followers = [ ... ]
        # tuple (t, c, k) where
        #   t = vector of knots
        #   c = b-spline coefficients
        #   k = degree of the spline
        tck = interpolate.splrep(timestamps, num_followers)

        t = ... # get post timestamp
        followers_for_timestamp = interpolate.splev(t, tck).item()
        # TODO: min(followers_for_timestamp, MIN_FOLLOWER_COUNT)
        #   (in case the interp goes negative)

      ** adds a dependency on scipy

    - order: (num_likes * normalized_comments) / follower_count
    - add table to instagram database:
        followers(
                timestamp REAL NOT NULL PRIMARY KEY,
                num_followers INTEGER NOT NULL
        )
      to interpolate follower counts from
    - update README dependencies (scipy)

- lower memory footprint

- hide/obfuscate requestor sensitive data logging
    eg. api secret keys, passwords, etc

- refactor config to be a static class
- refactor useragent to static constant (since it's program-wide)

? download user's media content & upload to imgur(?) (upload vids to gfycat)
    * handle case where API keys are banned
        > response will probably be 403
        - also need to handle instagram ban

    - make uploaders thread safe
        - gfycat
        - imgur

    O. gfycat albums
        O. how to create so that they can be shared though?
            >> hit /me/albums/{albumId}/name endpoint

    - gfycat ratelimiting
        - 1000 requests / rolling 24h
            > treat all requests the same
        - leave buffer for highlight uploads (backlog uploads should never
            eat into the buffer)

    - test imgur/gfycat
        - ratelimit (mock request call & fake ratelimit; check that request
                call is NOT made)

    ? reupload DMCA'd gfycats
        - check 'copyrightClaimant' on gfycats whenever doing a
            'GET .../me/albums/{albumId}' request

    * how to handle private users that have links for?
        - what about case where user goes private before all known media can be
            reuploaded?

    - databases:
        - per IG user imgur hashes + album hash
        - per IG user gfycat IDs + album ID
        - in progress imgur/gfycat uploads
            (in case the bot is restarted so it can check if uploads finish)
        - InstagramDatabase should no longer remove dead codes
            (should probably flag them as dead, maybe?)
        * alternatively: add imgur, gfycat tables to InstagramDatabase ?
            > probably want to leave cache table as is
              and add table:
                rehosted(
                    code TEXT NOT NULL,
                    rehosted_code TEXT NOT NULL,
                    hash TEXT NOT NULL,
                    is_video INTEGER NOT NULL,
                    available INTEGER NOT NULL,
                    dmca INTEGER NOT NULL DEFAULT 0,
                    fetch_url TEXT NOT NULL PRIMARY KEY
                )
              where:
                rehosted_code = gfyname or imgur hash
                hash = gfyid (lowercase gfyname) or imgur deletehash
                is_video = {0,1} whether media is a video
                available = {0,1} whether instagram post is still available
                dmca = {0,1} whether rehosted content was taken down due to
                            DMCA
                fetch_url = media url

    - priority instagram fetching
        - user data fetching = first priority
            * should no longer remove
            ? only fetch N media past last seen code
                this would mean that ordering is no longer 100% accurate since
                not all data would be re-fetched
        - highlights media fetching = medium prio
        - exhaustive media fetching = lowest prio

    - instagram media fetching
        - need to fetch all codes individually and get all sources in case
            there are multiple media in a post
            > images: remove_duplicates( ... 'display_url' )
            > videos: remove_duplicates( ... 'video_url' )

    - create two new log levels:
        - database - should be the lowest level
        - requests(?), maybe util(?) - should be higher than database,
            lower than debug

    - instagram video url fetching
        1. fetch instagram video code urls -> parse
            url = .find('meta', property='og:video:secure_url')['content']
        2. gfycat.Uploader(...).upload()

    - keep database of downloaded urls per user
        > should probably be separate from InstagramDatabase cache
    - keep database of imgur image/album hashes so that albums can be updated
    - imgur uploading process with imgur upload queue database
        - upload non-uploaded user media if near reset time
            - they do not explicitly respond with the ratelimit per-day reset
                > is it at a constant time (eg. 00:00:00) or based on the first
                    request (t + 24h)?
                >> appears to be relative to first request
            - leave a buffer of number of total requests per day
                'X-RateLimit-ClientRemaining' > N => upload backlog
            * -*Remaining headers seem to be off-by-one (too many)
                (they return the value before processing the request)
        - set album once backlog for user is completed

    https://apidocs.imgur.com
    https://github.com/Imgur/imgurpython

    - image upload with description:
        [link to gfycat if video]
        [caption]

        [date] ([link])

    * upload only 25 images/videos per user -- define in cfg?
        (imgur has a ~1250 upload/day ratelimit)
    - fallback to linking instagram codes if ratelimited
        - leave a note in reply saying:
            ^*imgur upload processing (may take a few days)*

    - create album if user album doesn't exist
        - title='full_name (@username)'
        - description='biography (external_url)'
        - privacy='hidden'
        ? cover='profile_pic'
    - update album if album metadata changed
        (same as create but update with new values)
    - set album with proper order if new media or expired

    > how to handle files > 10mb?
        - downsize if image
        - truncate down to 10mb
            > diff = os.path.getsize(name) - 1e7
            > if diff > 0: fd.truncate(1e7)
            (check if > because .truncate will increase the size if <)
    > can videos be uploaded directly to imgur? (yes for web interface)
    - fallback to linking behavior if no client_id, client_secret defined

? include version in FOOTER? (probably just the {major}.{minor} if yes)
? push to github
    - if no: remove [Source](...) from FOOTER
           ? remove link from FAQ
? fetch instagram data in a separate process
    would allow quicker replies of cached users
    - poll? callback? in replier to see if data is ready
O. remove try/catch in mentions/messages _run_forever
? target only python3
    there's no real reason to support both
? refactor queue databases (ig_queue, reddit_ratelimit, submission_queue) to
  use a base QueueDatabase class?
? block instagram users?
    would need a way to verify owner of instagram account so that random
    people can't cause the bot to no longer post highlights of instagram users
    - maybe just add a manual way to blacklist instagram user accounts

N. testing!

