----- STREAM OF CONSCIOUSNESS TODO FILE --------------------------------
The contents of this file may not make sense to anyone (including me).
Proceed at your own peril.
------------------------------------------------------------------------


O. add multiprocessing.RLock to blacklist database calls
    O. remove database/* locks
O. instead of communicating with messages through a queue,
    just pass self.blacklist & have messages make blacklist calls
O. refactor Reddit instantiation
    O. move to reddit.py (maybe rename redditprefix -> reddit)
    O. refactor bot.py:_handle_rate_limit -> reddit.py
        O. create a generic error handler which calls the pertinent
          error-handler (map APIException handlers by error_type)
        O. need to notify all processes of rate-limit & put all to sleep
    O. instantiate in bot.__init__
    O. instantiate another instance in messages.run_forever
O. add config options:
    O. SEND_DEBUG_PM on/off
        O. refactor into separate class? (in replies.py maybe?)
            -- needs to instantiate its own Reddit instance (to get the
            redditor object)
        ? add more debug pms
X. move messages.py reply methods to replies.py .. maybe
    I don't think anything else should need those specific methods
    (but the same can probably said of the methods in replies.py)
O. mentions parsing
    O. queue.put( thing.submission )
    O. gate by time -- don't parse through the same submission within
        T time
          but there could be new comments with usable links
          someone could spam mentions in a single submission
            eg. hitting enter a bunch of times because of lag
        O. maybe keep a database of (username, submission) & only
            queue.put if the specific combination has not been seen
            (prevents the same user summoning multiple times)
O. refactor messages, mentions stream parsing to mixin/base class
    O. change ProcessBase -> ProcessMixin
    O. handle prawcore.exceptions.RequestException (internet hiccup, etc)
O. implement bot.subs
    O. read subreddits from database / json
    O. join subreddits with '+'
    O. update from database whenever the file mtime changes
        (this is required for mentions-added subreddits to work without
         bot restart)
O. refactor database to use context-management (with statements) & commit/rollback
O. instagram stuff
    O. fetching & caching (+config option to expire cache)
        O. parse data into usable format
        O. caching = save data to file .. maybe a database? json?
            O. sqlite3 caching = check against file mtime
    O. bad links db for temp bans (404s only?)
        => if #bad_links > THRESHOLD: temp ban user!
    O. if ig fetch was triggered by mentions (submission_queue)
        and ig fetch was successful (fetch ok or cached)
        then add subreddit to to-add db:
            if subreddit reaches a threshold of successful mention summons
            then add subreddit to subreddit.comment stream parsing (bot.py)
O. instagram rate-limit queue
    O. needs database (ig_user, comment.fullname, last_id*, timestamp**)
      *last_id may be null if rate-limited before any fetches were made
        otherwise this should be the last fetched id (data['items'][-1]['id'])
      **timestamp could also just be an INTEGER AUTOINCREMENT
        O. how to treat table as queue?
            > SELECT ... FROM ... ORDER BY timestamp ASC;
    O. instagram inserts on rate-limit + other?
    O. bot processes every so often? alternatively, instagram processes queue
      whenever under rate-limit (needs a way to call appropriate bot reply
      func tho... or could instantiate a Reddit instance i guess..)
        O. refactor instagram.__*rate_limit* methods to public staticmethods
          so that bot.py can see them & use them to process queue
        O. whatever handles consuming queue needs to handle that ig_user hitting
          rate-limit again:
            > UPDATE ... (last_id)
        O. on successful fetch:
            > DELETE ...
X. catch-up (parse through old (all?) messages maybe comments/mentions)
    in case the bot is offline for a while, this would catch messages to be
    processed
    - refactor messages to fetch until first not-seen message
        (I think stream_generator only fetches the first 100 then continually
         tries to fetch the newest)
    ? refactor mentions/comments similarly
        this behavior may not be desirable; could end up responding to weeks-old
        comments to be seen by no one
O. commandline arguments
    . --config-path
    . --add-subreddit
    . --rm-subreddit
    . --add-blacklist
    . --rm-blacklist
    . --lookup-*
        database lookups; not sure which atm .. maybe all?
10? delay comment replies with persistent queue
    it seems like some people dislike bots because they reply with information
    that is useless to them. delaying the reply could potentially mitigate some
    hate.
X. don't dynamically add subreddits if they are blacklisted
    >>> this cannot happen (reply must occur in order for a subreddit to be
            added)
O. instagram requests user-agent
O. versioning
O. refactor logging
    O. add pid
    ? special config handling required?
    - bare-bones formatter
        need to refactor handle_special_keyword so that Formatter child classes
        can extend/override
    O. actually refactor logger calls in app
O. refactor reddit rate-limit handling to queue callbacks rather than sleeping
    this way, the bot will miss less comments to reply to
    assumption: POST/GET are separate rate-limits; ie, replying may be rate-
        limited but can still fetch new comments.
    O. persistent queue (adds a lot of complexity for a rare situation)
        requires a larger refactor (need to queue comment ?)
        X. plug into ig-queue
        O. table(thing_id, body_text, rate_limit_expire_time_utc)
        O. insert on rate-limit (or if rate-limit is active)
            (update if already queued -- ie, hit rate-limit again)
        O. delete on successful processing
        X. need in-memory queue?
    O. need to handle hitting rate-limit again from queue
    O. does this make error_handling obsolete? yes
O. logger: prepend msg with exc_info type name, ie:
            '.'.join([ exc_info.__module__, exc_info.__class__.__name__ ])
    > ideally, after any id information
O. __metaclass__ py2/3 compatibility
    (added six dependency which I think requests already depends on)
O. split config into sections
    O. write default config in repo
    O. read default config & merge with existing (if exists) & write config
        basically, don't squash existing options but do merge in newly defined
        options (and create a default config if it doesn't exist)
O. remove (all?) database paths from config
    why even allow the user to change these?
O. dynamically update config settings
    O. config.reload (check mtime vs cached mtime)
O. write pids to RUNTIME_ROOT_DIR
    O. os.makedirs(RUNTIME_ROOT_DIR)
    O. check if pid file(s?) exist before starting processes
    O. modify readme: 'mac' -> 'osx' & 'Application Support' -> 'Preferences'
    X. defer reddit login
O. initialize logger from config / args
    O. initialize immediately to streamhandler
    O. once config loaded, initialize with config settings
    O. no-color logging setting
    O. check config logging level is valid
        X. set up level lookup dictionary in logger/methods
O. fix python2 database (+more?) import
    dies on blacklist trying to import reddit
O. --dump: handle instagram databases
O. README:
    O. include section explaining config time parsing
    O. explain what EMAIL is used for (instagram user-agent string)
    O. change 'ig user\'s' -> 'instagram user\'s'
    O. change praw '> 4.0' -> '>= 4.0'
    O. document all locations that the program stores files
O. --remove-all-files (or similar)
    delete all stored files (ie, uninstall)
O. change submission_queue from Queue -> database
    in case the program terminates before the queue can be processed
O. catch UniqueConstraintFailed
    grep -iE '[.]insert\(|[.]update\(' -R *
O. use logger {yesno} keyword
O. reclassify logger.exceptions
O. graceful exit
    O. logging.shutdown
    X. database.close? .flush?
O. refactor queue processing to separate process so that comment streams are
  never interrupted (also in case comment streams always have new data --
  ie, no pause in comment streams)
    O. turn bot, mentions processes into queue producers which filter comments
      that we want to reply to (ie, that have instagram user links & can_reply)
        O. refactor can_reply logic to separate module/class
            O. pass/instantiate reply_history db in mentions proc
                ... or maybe in whatever houses can_reply
            O. make enqueue() method (called by bot and mentions)
            O. make get() method (called by consumer)
        O. refactor submission_queue (probably no longer needed)
    O. create new process which consumes queue:
      (this process should be the only one handling outbound replies so that
       the inbound streams are never interrupted)
        O. do_reply
        X. process ig_queue if reply_queue empty
        O. dynamically add subreddits
        O. temporarily ban bad actors
            O. mentions process needs to increment bad actors if summoned to
              post with no instagram links
                (possible false positive: comment contains non-hyperlinked
                 links -- eg. 'instagram: @foo.bar')
        ? send debug pms?
O. catch "sqlite3.OperationalError: database is locked"
    this occurs when 2+ procs attempt to execute on the same database at the
    same time (thrown on timeout I think)
    X. reraise as DatabaseIsLocked or something
        >> just auto-retry until it gets the lock
    O. double check any database that auto-updates is catching:
        O. IntegrityError
        O. OperationalError (with _db: ...)
            O. catch on _database level (loop until successful I guess)

O. why do reddit's rate_limit_time.value and ratelimit's rate_limit_time.value
    differ?
    > reddit kept trying to queue the reply => (value - now) <= 0
    >> ratelimit time remaining calculation was backwards

O. instantiate reddit instances in separate process (for debugging)
O. persistent reddit ratelimit time
    O. write end time to file and read in on init
    O. if time.time() > value => remove file
      else set rate_limit_time.value
? maximum depth of reply-able comment
    ie, only reply to comments where c.depth <= max_depth. typically
    very deep comments are only seen by the author & the person they were
    replying to (and maybe a handful of others); bot replies are probably not
    welcome unless specifically summoned.
? check comment's immediate replies in Filter to guarantee that the bot does not
    make duplicate replies regardless of database state
O. gracefully exit instagram fetch
    O. pass _killed flag to Instagram class? __init__?
    O. in fetch, if _killed.is_set() => set do_enqueue
- figure out why messages stream sleeps upon fetching items
    is it only if #items < 100?
O. reclassify some debug logging -> info (info is far too quiet)
- add the bot-posted comment id to the [Contact] msg subject skeleton
    - get comment from thing.reply()
    - change CONTACT_SUBJECT_SKELETON to include a comment id placeholder
        (ie, '{0}')
    - comment.edit( ... )
        probably: comment.edit( comment.body.format(comment.id) )
    * what if ratelimited on .edit ?
O. quiet spammy logging
    X. reddit-queue.db
        X. SELECT * FROM queue ORDER BY ratelimit_reset ASC
        X. Sleeping timeout=...
    X. reply-queue.db SELECT count(*)
    >> handled at base Database level by limiting the elapsed time between
        the same logged query.
- Fetch all(?) messages/mentions on first run and stop on first duplicate
    (to prevent missing older items)
O. delete negative threshold comments (ie, downvote to remove bot comments)
    O. fetch bot's comments:
        _reddit.user.me().comments.controversial(time_filter='month')
        .. maybe time_filter='week'
    O. call comment.delete() if comment.score < threshold
    O. define delete threshold in config
    O. dynamically change delay based on if any comments had negative score
        - saw non-deleted negative: delay = lower =>  O(minutes) ?
        - all comment.scores > 0:   delay = higher => O(hours) ?
    ! DO NOT modify reply database (don't want to reply again)
O. Write missing config options on startup
O. write help post & link appropriately in code (formatter.py)
O. change replyable_usernames to return list to preverve order
O. include choices in --help (add to help='...' msg)
- handle database table restructuring without dropping all data
O. parse soft-linked users
    O. in comments: @foo.bar
    X. in submission title: @foo.bar
        > requires another database?
            need to prevent reparsing same submission constantly
        >> too much extra overhead for no real gain
            99% of submissions don't soft-link anyway
    O. disable 404 badactor flagging since the bot is now guessing at usernames
        .. basically badactor flagging is turned off completely after this
O. test Parser.ig_{links,usernames}
    O. create script that takes comment_id & filename to create pickles
        O. c = Comment(comment_id)
        O. c.body
        O. with open('filename.py{ver}.pickle', 'wb') as fd:
            pickle.dump(c, fd)
    O. with usernames
    O. without usernames
    O. with links
    O. without links
    O. without either
O. add num_comments column to InstagramDatabase
    want a way to include #comments in sort but not all media has comments

    X. highlights = ORDER BY num_likes DESC, num_comments DESC
        should get highest liked & most commented media first (ie, most popular)
        >> this would just sort by num_likes anyway ...
    ? include time so that new posts that are very popular are sorted correctly
        - cap the impact of time at ~3d?
            (so that very old posts that were popular aren't sorted to the end)
    O. change args.py --ig-db ORDER BY to reflect new order
    O. change FAQ, bot side-bar "top-liked" -> "most popular"

O. ignore AutoModerator '@username' soft-links
    ! hard-links ok
    O. fix IG_USER_REGEX to match '(@username)'
    O. in ig_usernames: skip if comment.author.name.lower() == 'automoderator'
        or better: ignore if .name.lower() in IGNORE_LIST
    O. test: parenthesis_username.pickle

- determine why there is a 3 ~ 5s delay between the Replier removing the comment
    from the repy-queue and calling ._reply() when instagram data is fetched
    (just looking up from database does not incur this delay)
    > is this delay system dependant?
X. reply-queue per-comment logging
    > how to prevent spam the log though?
    >> handled in 821f2ea (kind of)
? refactor ratelimit flag process -> thread
    > do multiprocessing.Events work in threads?
        need to test; I assume yes
? logger keyword: {pluralize}
    - take tuple? list? needs the actual word value (eg. 'entry') and the
        corresponding num kwarg to test if the word should be plural
    - if ends in 'y':
        - check word[0] == upper => 's'
        - check word[-2] != vowel => 'ies'
    (in case a database is changed significantly)
- fix instagram 500-level codes spamming log
- turn off Parser._get_potential_user_strings() if bot is scraping a popular
  subreddit
    - how to classify a subreddit as popular?
        > maybe average(?) time between comments?
? move instagram urls to separate file?
- refactor instagram to package
    the file is getting too large
    - ratelimit handler
    - fetch handler
    ? queue handler
    ? database handler
? refactor parser to package ?
? store last controversial check iteration time/delay in file
    so that program restarts don't re-trigger an early controversial check
O. --shutdown or --kill or something so that the program can be terminated from
    any directory/terminal
O. why is 'Daring' considered jargon?
O. match 's?he\'s {username} on insta'
    X. make prefix/suffix a single list of fmt
    X. make helper func that string.formats each fmt in list for
        HAS_IG_KEYWORD, IG_USER_STRING
    >> broadened how matching works

- fix instagram 500-level code delay logging spam
    > can't handle at replier level because users may be cached
        but sort of have to handle there since spam starts at that level

O. try finding usernames per line
    O. [IG_USER_STRING_REGEX.search(text.strip()) for text in comment.body.split('\n')]
        X. filter(None, ...)
        >> no need
O. parse submission titles for possible instagram user strings
    O. parse submission .url?
    O. parse submission .selftext?
    O. parse submission .title
    O. submissions process?
        > can't rely on every submission having a comment
    X. need database to store submissions
        >> just re-parse; shouldn't be a huge issue
    O. store replies properly
        ? refactor replies.db ?
            > I think the column name, comment_fullname, is wrong but should
                work besides that
    O. update faq
    O. update sidebar
    O. update readme
    O. update app description
- parse submission from mentions
? parse comments' submissions from comment stream if not parsed yet?
? separate submissions/comments min length for potential username matches
- dynamically update logging level/color (maybe path?) when config is reloaded
- add most liked, most commented links to reply header
    > how to handle duplicates?
        1. most liked == most commented
            - fetch most liked from db
            - fetch most commented with exclusions from db
                (exclude=most_liked_code)
        2. either/both already in highlights set
            - fetch top_media exclude=(most_liked, most_commented)
O. learn "bad" words so that they are not tried again
    O. prevent previously deleted comment users from being matched as
        potential usernames in the future (links still ok)
        > what about '@'s?
            1. non-instagram user soft-link
            2. are there other cases?
            >> previous bad usernames will prune any non-link username
        O. ''.join(map(lambda x: x+'+', bad_username))
            eg. 'foobar' -> 'f+o+o+b+a+r+'
            >> may as well match against these
X. --dry-run Controversial
    X. don't delete thing
    >> the bot should probably delete regardless of mode
- fix test_parser creating actual databases
- blacklist subredits the bot has been banned from
    - read SubredditMessages saying "banned from /r/..."
- test stale ig queue
- test replies
    - test comments are stored properly
    - test submissions are stored properly
- test ratelimit still works
- test mentions
- test messages

? include version in FOOTER? (probably just the {major}.{minor} if yes)
? push to github
    - if no: remove [Source](...) from FOOTER
           ? remove link from FAQ
? fetch instagram data in a separate process
    - poll? callback? in replier to see if data is ready
O. remove try/catch in mentions/messages _run_forever
? target only python3
    there's no real reason to support both
? refactor queue databases (ig_queue, reddit_ratelimit, submission_queue) to
  use a base QueueDatabase class?
? block instagram users?
    would need a way to verify owner of instagram account so that random
    people can't cause the bot to no longer post highlights of instagram users
    - maybe just add a manual way to blacklist instagram user accounts

N. testing!

