----- STREAM OF CONSCIOUSNESS TODO FILE --------------------------------
The contents of this file may not make sense to anyone (including me).
Proceed at your own peril.
------------------------------------------------------------------------


O. add multiprocessing.RLock to blacklist database calls
    O. remove database/* locks
O. instead of communicating with messages through a queue,
    just pass self.blacklist & have messages make blacklist calls
O. refactor Reddit instantiation
    O. move to reddit.py (maybe rename redditprefix -> reddit)
    O. refactor bot.py:_handle_rate_limit -> reddit.py
        O. create a generic error handler which calls the pertinent
          error-handler (map APIException handlers by error_type)
        O. need to notify all processes of rate-limit & put all to sleep
    O. instantiate in bot.__init__
    O. instantiate another instance in messages.run_forever
O. add config options:
    O. SEND_DEBUG_PM on/off
        O. refactor into separate class? (in replies.py maybe?)
            -- needs to instantiate its own Reddit instance (to get the
            redditor object)
        ? add more debug pms
X. move messages.py reply methods to replies.py .. maybe
    I don't think anything else should need those specific methods
    (but the same can probably said of the methods in replies.py)
O. mentions parsing
    O. queue.put( thing.submission )
    O. gate by time -- don't parse through the same submission within
        T time
          but there could be new comments with usable links
          someone could spam mentions in a single submission
            eg. hitting enter a bunch of times because of lag
        O. maybe keep a database of (username, submission) & only
            queue.put if the specific combination has not been seen
            (prevents the same user summoning multiple times)
O. refactor messages, mentions stream parsing to mixin/base class
    O. change ProcessBase -> ProcessMixin
    O. handle prawcore.exceptions.RequestException (internet hiccup, etc)
O. implement bot.subs
    O. read subreddits from database / json
    O. join subreddits with '+'
    O. update from database whenever the file mtime changes
        (this is required for mentions-added subreddits to work without
         bot restart)
O. refactor database to use context-management (with statements) & commit/rollback
O. instagram stuff
    O. fetching & caching (+config option to expire cache)
        O. parse data into usable format
        O. caching = save data to file .. maybe a database? json?
            O. sqlite3 caching = check against file mtime
    O. bad links db for temp bans (404s only?)
        => if #bad_links > THRESHOLD: temp ban user!
    O. if ig fetch was triggered by mentions (submission_queue)
        and ig fetch was successful (fetch ok or cached)
        then add subreddit to to-add db:
            if subreddit reaches a threshold of successful mention summons
            then add subreddit to subreddit.comment stream parsing (bot.py)
O. instagram rate-limit queue
    O. needs database (ig_user, comment.fullname, last_id*, timestamp**)
      *last_id may be null if rate-limited before any fetches were made
        otherwise this should be the last fetched id (data['items'][-1]['id'])
      **timestamp could also just be an INTEGER AUTOINCREMENT
        O. how to treat table as queue?
            > SELECT ... FROM ... ORDER BY timestamp ASC;
    O. instagram inserts on rate-limit + other?
    O. bot processes every so often? alternatively, instagram processes queue
      whenever under rate-limit (needs a way to call appropriate bot reply
      func tho... or could instantiate a Reddit instance i guess..)
        O. refactor instagram.__*rate_limit* methods to public staticmethods
          so that bot.py can see them & use them to process queue
        O. whatever handles consuming queue needs to handle that ig_user hitting
          rate-limit again:
            > UPDATE ... (last_id)
        O. on successful fetch:
            > DELETE ...
X. catch-up (parse through old (all?) messages maybe comments/mentions)
    in case the bot is offline for a while, this would catch messages to be
    processed
    - refactor messages to fetch until first not-seen message
        (I think stream_generator only fetches the first 100 then continually
         tries to fetch the newest)
    ? refactor mentions/comments similarly
        this behavior may not be desirable; could end up responding to weeks-old
        comments to be seen by no one
O. commandline arguments
    . --config-path
    . --add-subreddit
    . --rm-subreddit
    . --add-blacklist
    . --rm-blacklist
    . --lookup-*
        database lookups; not sure which atm .. maybe all?
10? delay comment replies with persistent queue
    it seems like some people dislike bots because they reply with information
    that is useless to them. delaying the reply could potentially mitigate some
    hate.
X. don't dynamically add subreddits if they are blacklisted
    >>> this cannot happen (reply must occur in order for a subreddit to be
            added)
O. instagram requests user-agent
O. versioning
O. refactor logging
    O. add pid
    ? special config handling required?
    - bare-bones formatter
        need to refactor handle_special_keyword so that Formatter child classes
        can extend/override
    O. actually refactor logger calls in app
O. refactor reddit rate-limit handling to queue callbacks rather than sleeping
    this way, the bot will miss less comments to reply to
    assumption: POST/GET are separate rate-limits; ie, replying may be rate-
        limited but can still fetch new comments.
    O. persistent queue (adds a lot of complexity for a rare situation)
        requires a larger refactor (need to queue comment ?)
        X. plug into ig-queue
        O. table(thing_id, body_text, rate_limit_expire_time_utc)
        O. insert on rate-limit (or if rate-limit is active)
            (update if already queued -- ie, hit rate-limit again)
        O. delete on successful processing
        X. need in-memory queue?
    O. need to handle hitting rate-limit again from queue
    O. does this make error_handling obsolete? yes
O. logger: prepend msg with exc_info type name, ie:
            '.'.join([ exc_info.__module__, exc_info.__class__.__name__ ])
    > ideally, after any id information
O. __metaclass__ py2/3 compatibility
    (added six dependency which I think requests already depends on)
O. split config into sections
    O. write default config in repo
    O. read default config & merge with existing (if exists) & write config
        basically, don't squash existing options but do merge in newly defined
        options (and create a default config if it doesn't exist)
O. remove (all?) database paths from config
    why even allow the user to change these?
- dynamically update config settings
    - config.reload (check mtime vs cached mtime)
O. write pids to RUNTIME_ROOT_DIR
    O. os.makedirs(RUNTIME_ROOT_DIR)
    O. check if pid file(s?) exist before starting processes
    O. modify readme: 'mac' -> 'osx' & 'Application Support' -> 'Preferences'
    X. defer reddit login
O. initialize logger from config / args
    O. initialize immediately to streamhandler
    O. once config loaded, initialize with config settings
    O. no-color logging setting
    O. check config logging level is valid
        X. set up level lookup dictionary in logger/methods
O. fix python2 database (+more?) import
    dies on blacklist trying to import reddit
O. --dump: handle instagram databases
O. README:
    O. include section explaining config time parsing
    O. explain what EMAIL is used for (instagram user-agent string)
    O. change 'ig user\'s' -> 'instagram user\'s'
    O. change praw '> 4.0' -> '>= 4.0'
    O. document all locations that the program stores files
O. --remove-all-files (or similar)
    delete all stored files (ie, uninstall)
O. change submission_queue from Queue -> database
    in case the program terminates before the queue can be processed
O. catch UniqueConstraintFailed
    grep -iE '[.]insert\(|[.]update\(' -R *
O. use logger {yesno} keyword
O. reclassify logger.exceptions
O. graceful exit
    O. logging.shutdown
    X. database.close? .flush?
O. refactor queue processing to separate process so that comment streams are
  never interrupted (also in case comment streams always have new data --
  ie, no pause in comment streams)
    O. turn bot, mentions processes into queue producers which filter comments
      that we want to reply to (ie, that have instagram user links & can_reply)
        O. refactor can_reply logic to separate module/class
            O. pass/instantiate reply_history db in mentions proc
                ... or maybe in whatever houses can_reply
            O. make enqueue() method (called by bot and mentions)
            O. make get() method (called by consumer)
        O. refactor submission_queue (probably no longer needed)
    O. create new process which consumes queue:
      (this process should be the only one handling outbound replies so that
       the inbound streams are never interrupted)
        O. do_reply
        X. process ig_queue if reply_queue empty
        O. dynamically add subreddits
        O. temporarily ban bad actors
            O. mentions process needs to increment bad actors if summoned to
              post with no instagram links
                (possible false positive: comment contains non-hyperlinked
                 links -- eg. 'instagram: @foo.bar')
        ? send debug pms?
O. catch "sqlite3.OperationalError: database is locked"
    this occurs when 2+ procs attempt to execute on the same database at the
    same time (thrown on timeout I think)
    X. reraise as DatabaseIsLocked or something
        >> just auto-retry until it gets the lock
    O. double check any database that auto-updates is catching:
        O. IntegrityError
        O. OperationalError (with _db: ...)
            O. catch on _database level (loop until successful I guess)

O. why do reddit's rate_limit_time.value and ratelimit's rate_limit_time.value
    differ?
    > reddit kept trying to queue the reply => (value - now) <= 0
    >> ratelimit time remaining calculation was backwards

O. instantiate reddit instances in separate process (for debugging)
- persistent reddit ratelimit time
    - write end time to file and read in on init
    - if time.time() > value => remove file
      else set rate_limit_time.value
- maximum depth of reply-able comment
    ie, only reply to comments where c.depth <= max_depth. typically
    very deep comments are only seen by the author & the person they were
    replying to (and maybe a handful of others); bot replies are probably not
    welcome unless specifically summoned.
- check comment's immediate replies in Filter to guarantee that the bot does not
    make duplicate replies regardless of database state
- reclassify some debug logging -> info (info is far too quiet)
- quiet spammy logging
    - reddit-queue.db
        - SELECT * FROM queue ORDER BY ratelimit_reset ASC
        - Sleeping timeout=...
    - reply-queue.db SELECT count(*)
- write help post & link appropriately in code (formatter.py)
? include version in FOOTER? (probably just the {major}.{minor} if yes)
? push to github
    - if no: remove [Source](...) from FOOTER
? fetch instagram data in a separate process
    - poll? callback? in replier to see if data is ready
O. remove try/catch in mentions/messages _run_forever
? target only python3
    there's no real reason to support both
? refactor queue databases (ig_queue, reddit_ratelimit, submission_queue) to
  use a base QueueDatabase class?

N. testing!

